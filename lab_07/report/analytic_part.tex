\chapter{Аналитическая часть}

\section{Регулярные выражения}

Регулярное выражение — это последовательность специальных символов, формирующих паттерн или шаблон, который сопоставляется со строкой. Целью такого сопоставления является определение соответствия значения в строке шаблону для её валидации. Регулярные выражения являются регистрозависимыми~\cite{regular}.

\section{Большие языковые модели}

Большая языковая модель (LLM) — это продвинутая вычислительная модель на базе нейронных сетей, способная анализировать, генерировать тексты и выявлять сложные закономерности в языковых данных. Эволюция этих систем прошла путь от простых задач лексического перевода в 1990--х до появления в 2017 году революционной архитектуры Transformer, которая легла в основу самых популярных современных моделей, таких как GPT, BERT и T5. Эти технологии позволили внедрить искусственный интеллект практически во все сферы бизнеса и науки, от автоматизации поддержки клиентов до разработки лекарств.

Принцип работы LLM строится на сложной последовательности этапов, выполняемых за доли секунды. Сначала запрос пользователя проходит токенизацию (разбивку на части) и векторизацию, превращаясь в математические эмбеддинги, понятные машине. Затем эти данные проходят через многочисленные слои нейросети, где модель формирует ответ на основе своей базы знаний, применяет фильтры безопасности и декодирует результат обратно в текст. Пользователь может влиять на характер ответа с помощью промпт-инжиниринга и настройки параметров, таких как «температура», определяющая степень креативности генерации.

Несмотря на высокую эффективность, перед разработчиками LLM стоит ряд серьезных вызовов. К ним относятся <<галлюцинации>> (генерация фактологически неверной информации), этические проблемы, связанные с предвзятостью данных, и сложности с интерпретацией глубокого контекста. Кроме того, современные модели чрезвычайно ресурсоемки: их обучение и работа требуют огромных вычислительных мощностей и электроэнергии, что стимулирует поиск новых, более экономичных архитектур, таких как RNN и Mamba~\cite{llm}.

Для выполнения данной лабораторной работы были выбраны следующие БЯМ, так как они являются локально разворачиваемыми (open-weight), а также одними из одними из самых лучших БЯМ по критерию <<интеллект>>~\cite{artificialanalysis}:

\begin{enumerate}
	\item Kimi K2 Thinking,
	\item DeepSeek V3.2,
	\item GLM-4.6.
\end{enumerate}

\section{Kimi K2 Thinking}

Kimi K2-Thinking — это специализированная большая языковая модель от компании Moonshot AI, разработанная для выполнения задач, требующих глубокого рассуждения и сложной логики. В отличие от стандартных чат-ботов, которые стремятся дать мгновенный ответ, эта модель следует парадигме «медленного мышления» (System 2 thinking), аналогичной подходу OpenAI o1 и DeepSeek R1. Перед генерацией финального ответа она создает развернутую цепочку внутренних рассуждений (Chain of Thought), что позволяет ей эффективно планировать решения, находить неочевидные взаимосвязи и проводить самопроверку в процессе работы. Одной из ключевых особенностей является способность модели прозрачно демонстрировать ход своих «мыслей», что критически важно для отладки и верификации результатов в научных и инженерных задачах.

Архитектурно Kimi Л2-Thinking представляет собой массивную модель на основе технологии Mixture-of-Experts (MoE) с общим количеством параметров, достигающим 1 триллиона. Однако благодаря разреженной структуре MoE, при обработке каждого токена активируется лишь около 32 миллиардов параметров (из 384 экспертов выбираются 8 наиболее релевантных). Это позволяет сочетать мощь гигантской модели с относительно высокой скоростью инференса и экономической эффективностью. Модель поддерживает внушительное контекстное окно в 256 000 токенов, что позволяет загружать в нее целые кодовые базы или объемные документы. Кроме того, для обучения использовался проприетарный оптимизатор MuonClip, а сама модель оптимизирована для работы с квантованием INT4 без существенной потери качества.

Функционально модель выделяется продвинутыми возможностями использования инструментов и агентского поведения. Она обучена с применением методов обучения с подкреплением (Reinforcement Learning), что позволяет ей не только генерировать текст, но и автономно выполнять последовательности из 200–300 вызовов внешних инструментов, чередуя их с этапами рассуждения. Это делает Kimi k2-Thinking особенно эффективной в задачах STEM (наука, технологии, инженерия, математика), при написании сложного программного кода и в сценариях, где требуется длительное удержание контекста и многошаговое планирование без потери логической нити~\cite{kimi}.


\section{DeepSeek V3.2}

DeepSeek-V3.2 представляет собой передовую открытую языковую модель, разработанную для устранения разрыва между вычислительной эффективностью и сложными способностями к рассуждению. Главным архитектурным прорывом модели стало внедрение механизма DeepSeek Sparse Attention (DSA). В отличие от стандартного механизма внимания, который страдает от квадратичной сложности при обработке длинных последовательностей, DSA использует <<молниеносный индексатор>> (lightning indexer) и селектор токенов для динамического выбора только наиболее важных ключей и значений. Это позволяет снизить вычислительную сложность, сохраняя высокую производительность даже в контекстных окнах длиной до 128 000 токенов, и делает модель значительно более экономичной при инференсе по сравнению с предыдущими версиями.

Процесс пост-тренировки модели претерпел значительные изменения благодаря масштабируемому фреймворку обучения с подкреплением (RL). Разработчики увеличили вычислительный бюджет этапа RL до более чем 10\% от стоимости предварительного обучения, используя алгоритм GRPO (Group Relative Policy Optimization). Для улучшения агентных способностей был создан конвейер синтеза задач, сгенерировавший тысячи виртуальных сред и десятки тысяч сложных промптов. Это позволило модели научиться интегрировать процесс глубокого <<мышления>> (Chain of Thought) непосредственно в сценарии использования внешних инструментов, что ранее было слабым местом открытых моделей. Теперь модель может сохранять логику рассуждений между вызовами инструментов, не теряя контекст~\cite{deepseek}.

\section{GLM-4.6}

GLM-4.6 является улучшением модели GLM-4.5 с той же архитектурой. GLM-4.5 --- это  большая языковая модель архитектуры Mixture-of-Experts (MoE), разработанная командой Zhipu AI и Университетом Цинхуа. Модель насчитывает 355 миллиардов общих параметров, из которых 32 миллиарда являются активными при каждом токене. Ключевой особенностью GLM-4.5 является гибридный метод рассуждений, который поддерживает как режим <<мышления>> для решения сложных задач, так и режим прямого ответа. Эта модель позиционируется как универсальный инструмент, объединяющий три критические способности (ARC): агентные навыки (Agentic), глубокие рассуждения (Reasoning) и программирование (Coding).

Процесс обучения GLM-4.5 включал тренировку на колоссальном объеме данных в 23 триллиона токенов, разделенную на несколько этапов: предварительное обучение, mid-training (для улучшения работы с длинным контекстом до 128K и специализации) и пост-тренировку. В архитектуре используется оптимизатор Muon и сигмоидные шлюзы (sigmoid gates) для маршрутизации экспертов. Особое внимание уделялось пост-тренировке с использованием итеративного самообучения и масштабируемого обучения с подкреплением (RL). Разработчики внедрили пайплайн, включающий <<холодный старт>> (Cold Start SFT) для инициализации способностей и последующий RL на основе алгоритма GRPO, который оптимизирует модель для выполнения сложных агентных задач, таких как использование веб-поиска и написание кода в реальных средах~\cite{glm}.


\section*{Вывод}

В аналитической части были рассмотрены определения регулярного выражения, большой языковой модели. Были выбраны и описаны 3 используемых БЯМ.
